{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical IX\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "9-20.12.2024\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will implement the training script train the transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3becedbe2f606",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Essentials of the training script\n",
    "\n",
    "In the training script the prepared dataset is used to train a model instance. Before training the model, the instance should be initialised, a dataloader should be created from the dataset and the loss function, optimiser and learning rate scheduler should be initialised. \n",
    "\n",
    "#### 1.1. Dataloaders\n",
    "\n",
    "The dataloader is a pytorch class that is used to load data from the dataset in batches. The dataloader is initialised with the dataset and the batch size. The dataloader is then used to iterate over the dataset in batches. The dataloader is used in the training loop to load the data for each batch.\n",
    "\n",
    "The dataloader for the training data is initialised as follows:\n",
    "\n",
    "```python\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "```\n",
    "\n",
    "#### 1.2. Training the model\n",
    "\n",
    "Once the model, dataloader and loss function are initialised, the model can be trained. The training loop iterates over the batches in the dataloader and performs the following steps:\n",
    "\n",
    "1. Load the data for the batch.\n",
    "2. Perform a forward pass through the model.\n",
    "3. Calculate the loss.\n",
    "4. Perform a backward pass through the model.\n",
    "5. Update the model parameters.\n",
    "6. Update the learning rate.\n",
    "7. Repeat for the next batch.\n",
    "8. Repeat for the next epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20b8711fe743b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Initialise a small version of your transformer model (do not use more than 4 layers and 64 hidden units unless you have access to sufficient compute).\n",
    "2. Initialise the dataloader using the dataset class from practical 5.\n",
    "3. Initialise the loss function (cross entropy loss), optimiser and learning rate scheduler.\n",
    "4. Implement the training loop.\n",
    "5. Train the model for 5 epochs and ensure that loss decreases for both the training and validation sets of the dataset. You can use a small randomly selected subset of the training data to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8e9c54ce28429",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
