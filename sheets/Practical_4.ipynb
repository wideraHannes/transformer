{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical IV\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "28-31.10.2024\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will explore the translation task, the dataset for translation and cleaning up and preparing the data for training a model as well as the word embedding layers. In the sessions this week we will discuss how to create the tokenizer class for pre-training and how to create dataset and dataloader objects. We will further explore the embedding layer, as well as the positional embedding layer (recall in practical 1 we discussed the importance of encoding the position of an element in a sequence). We will discuss the positional encodings and proofs in the practical sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf904ffe07eb02b7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Translation Task\n",
    "\n",
    "The translation task is a sequence to sequence task. The input is a sequence of words in one language and the output is a sequence of words in another language. The input and output sequences are not necessarily the same length. The input sequence is usually referred to as the source sequence and the output sequence as the target sequence.\n",
    "\n",
    "#### 1.1. Representing the translation task numerically\n",
    "\n",
    "Before building a neural network model for machine translation, it's crucial to understand how text data, a series of characters or words, is converted into a format that can be processed numerically by neural network models. Here's a brief overview:\n",
    "\n",
    "##### 1. Tokenization:\n",
    "\n",
    "This is the first step, where a piece of text is divided into smaller chunks, called tokens. For now we assume these tokens are words. For example, *\"Welcome to this practical series!\"* can be tokenized into `['welcome', 'to', 'this', 'practical', 'series', '!']`.\n",
    "\n",
    "##### 2. Building a Vocabulary:\n",
    "\n",
    "Once texts are tokenized, a vocabulary is constructed. This is a unique list of words found in the dataset. From our example, the vocabulary would be: `{'welcome', 'to', 'this', 'practical', 'series', '!'}`.\n",
    "\n",
    "##### 3.Encoding Words as Numbers:\n",
    "\n",
    "Each unique word in the vocabulary is assigned a unique numerical identifier. This allows us to convert textual data into a numerical format. Using our example: *'welcome'* might be represented as '0', *'to'* as '1' and so on. This results in the numerical representation `[0, 1, 2, 3, 4, 5]` for the sentence *\"Welcome to this practical series!\"*.\n",
    "\n",
    "By transforming text data into numerical form, sequence to sequence neural network models can process and learn from the data, enabling them to perform tasks such as machine translation.\n",
    "\n",
    "#### 1.2. The Dataset\n",
    "We will use the [WMT 17 German to English dataset](https://huggingface.co/datasets/wmt17/viewer/de-en/train) for this practical. This dataset contains 5.9 million sentence pairs. The dataset is available through the Huggingface datasets package and can be loaded as follows:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wmt17\", \"de-en\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f7f8c05af7cc3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Cleaning and Preparing the Data\n",
    "\n",
    "#### 2.1. Cleaning the Data\n",
    "\n",
    "Before we can use this dataset for training a model, we need to clean it. This involves removing sentences that are too long or too short, removing sentences that contain too many unknown tokens and removing sentences that contain too many non-alphabetic characters. We will also convert all sentences to lowercase.\n",
    "\n",
    "We will apply the following set of operations to clean the data:\n",
    "\n",
    "- Remove any non-UTF8 characters, URL's or HTML tags.\n",
    "- Remove characters not whitelisted to avoid injecting unnecessary characters into the model vocab. We will use the whitelist `\"abcdefghijklmnopqrstuvwxyz ÄÖÜäöüß ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?()[]{}:;-&$@#%£€/\\|_+*¥\"` for this.\n",
    "- Remove sentences that are too long or too short. We will use a minimum length of 5 and a maximum length of 64 for this.\n",
    "- Remove translation-pairs where the ratio between the source and target sentence is too large.\n",
    "\n",
    "#### 2.2. Building the Vocabulary of the Model\n",
    "\n",
    "Once the data is cleaned we can build the vocabulary of the model. The vocabulary should consists of the top V most frequent words in the dataset. We will use a vocabulary size of V=50000 for this practical (we will use a single vocabulary for both languages). The vocabulary should also contain special tokens for padding, unknown tokens and the start and end of a sentence `[PAD], [BOS], [EOS]`.\n",
    "\n",
    "#### 2.3. Encoding the Data\n",
    "\n",
    "Once the vocabulary is built, we can encode the data. This involves converting the text data into numerical form using the vocabulary. Here it is important to consider that in neural networks data is processed batches of sequences simultaneously for computational efficiency. However, real-world text sequences vary in length, and this presents a challenge: neural networks require inputs of a consistent shape and size.\n",
    "\n",
    "This is where padding tokens come into play. Padding ensures that all sequences in a batch share the same length by appending special <PAD> tokens to shorter sequences until they match the length of the longest sequence in the batch. This uniformity is crucial for matrix (and tensor) operations inside the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb5d29",
   "metadata": {},
   "source": [
    "### 3. Word Embeddings\n",
    "\n",
    "Word embeddings are a way of representing words as vectors. These vectors are learned during training, and are used to represent words in a way that captures their meaning. This is useful for many NLP tasks, as it allows us to represent words in a way that is more useful for machine learning models.\n",
    "\n",
    "Mathematically the word embedding layer is represented as:\n",
    "\n",
    " ${\\tt Emb}(\\mathbf{x}) = \\mathcal{1} (\\mathbf{x}) \\mathbf{E}$ \n",
    "\n",
    "where $\\mathbf{x}$ is a vector of word indices, $\\mathcal{1}$ is a one-hot encoding function, and $\\mathbf{E}$ is a matrix of word embeddings. The one-hot encoding function is a function that takes a vector of word indices, and returns a matrix where each row is a one-hot encoding of the corresponding word index. The word embedding matrix is a matrix where each row is the word embedding of the corresponding word index.\n",
    "\n",
    "The word embedding layer acts as a lookup table, where each row of the matrix is a word embedding. Multiplying the one-hot matrix with the embedding matrix is equivalent to selecting the word embedding of the word index. We will utilise the pytorch `Embedding` layer to implement this. (See [here](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) for more details.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741b35bb534bafe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Write a data cleaning function for the dataset.\n",
    "2. Create a tokenizer class, this class should be based on the huggingface BPE tokenizer used in Practical 4. (Hint: learn the vocabulary using the BPETokenizer class, convert the vocabulary and merges dict to the format required by the GPT2Tokenizer class and then create a GPT2Tokenizer object using the from_pretrained method).\n",
    "3. Write a torch dataset class to encode the dataset and store it. This class will be used to create dataloaders for training your model.\n",
    "4. Study the positional encoding layer proposed in the paper [Attention is all you need](https://arxiv.org/abs/1706.03762). Prove the properties of the positional encodings presented (i.e. for a fixed offset k the positional encodings $PE_{t+k}$ can be represented as a linear function of $PE_t$ and The wavelengths form a geometric progression from 2π to 10000*2π).\n",
    "5. Implement the positional encoding layer in pytorch.\n",
    "6. Implement the word embedding layer in pytorch.\n",
    "7. Test your positional encoding layer using the provided tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3eb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hanneswidera/Uni/Master/Semester 3/Transformer/transformer/gpt2_from_bpe\n"
     ]
    }
   ],
   "source": [
    "from config.paths import OUTPUT\n",
    "\n",
    "\n",
    "print(OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2df80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
