{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical II\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "14-18.10.2024\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will explore the two different reasons for masking in the attention mechanism and implement both. We will further discuss how deep learning code can be tested using the `pytest` framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3becedbe2f606",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Masking\n",
    "#### 1.1. Masking Padded Tokens\n",
    "\n",
    "In the previous practical, we delved into the implementation of the attention mechanism, which computes a weighted sum of values ($V$) given a query ($Q$) and a set of keys ($K$). The example illustrated the process of deriving contextual representations for each token in a character sequence based on an input sequence.\n",
    "\n",
    "In real-world scenarios, multiple sequences are often processed simultaneously, utilizing Tensors of shape (batch_size, sequence_length, embedding_size). However, a challenge arises as Tensors necessitate uniform sequence lengths, which is impractical given the natural variability in sentence lengths. A common resolution to this is padding the sequences to attain a standardized length. Padding entails appending a distinct token (e.g., <pad>) to the end of shorter sequences until uniformity in length is achieved across all sequences. For instance, given the sentences: \"Welcome to this tutorial on attention\" and \"Please remember to pad your reply before sending it\", the first sentence would be padded with the `<pad>` token to become: \"Welcome to this tutorial on attention `<pad> <pad> <pad>`\", facilitating the combination of these sentences into a Tensor with sequence_length = 9.\n",
    "\n",
    "When employing the attention mechanism to generate contextual representations for these sentences, it's crucial that the <pad> tokens do not alter the original \"meaning\" of the sentences. To ensure this, the padding tokens are masked, thereby instructing the attention mechanism to disregard the <pad> tokens during processing.\n",
    "\n",
    "#### 1.2. Future Masking\n",
    "\n",
    "In the model introduced in the paper Attention is all you need, a \"decoder\" component is featured. Within this decoder, the inputs are the target outcomes, each shifted upward by one position. A critical aspect of this decoder's functionality is the \"masking\" of future tokens to prevent the model from inadvertently \"cheating\" by peeking at the subsequent token in the sequence. This is accomplished by masking the upper triangular portion of the attention matrix, ensuring the attention mechanism remains oblivious to the future tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b83cd39df2035",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Testing\n",
    "\n",
    "Unit testing of the individual modules in a deep learning model is important. It ensures that the module perform the intended operations. In this practical we will use the `pytest` framework to test the implementation of the attention mechanism. To write a unit test for a simple linear layer, we can write the following test function:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "\n",
    "def test_linear_layer():\n",
    "    # Set seed for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Define linear layer\n",
    "    layer = Linear(2, 3)\n",
    "\n",
    "    # Generate random weight and bias vectors for the linear layer\n",
    "    weight = torch.randn(layer.weight.size())\n",
    "    bias = torch.randn(layer.bias.size())\n",
    "    layer.weight = torch.nn.Parameter(weight)\n",
    "    layer.bias = torch.nn.Parameter(bias)\n",
    "\n",
    "    x = torch.randn(5, 2)\n",
    "\n",
    "    # Compute the expected and actual outputs\n",
    "    expected = torch.matmul(weight, x.T).T + bias.unsqueeze(0).repeat((x.size(0), 1))\n",
    "    actual = layer(x)\n",
    "\n",
    "    assert torch.allclose(expected, actual)\n",
    "```\n",
    "\n",
    "The `test_linear_layer` function tests the linear layer by comparing the expected output with the actual output. The `assert` statement checks whether the two outputs are equal. If they are not equal, the test fails. The `torch.allclose` function checks whether the two tensors are equal within a certain tolerance. This is necessary because floating point operations are not always exact. The `torch.allclose` function returns a boolean tensor, which is `True` if the two tensors are equal within the specified tolerance and `False` otherwise. The `assert` statement checks whether all elements in the boolean tensor are `True`. If this is the case, the test passes. If not, the test fails.\n",
    "\n",
    "This test can be executed using:\n",
    "\n",
    "```bash\n",
    "pytest test_linear_layer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83c3ad854fabf7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Make sure you understand the role of the two different masks in the attention mechanism. Explain the role of each mask in your own words.\n",
    "2. Incorporate both masking mechanisms into your attention mechanism implementation from the preceding practical. For masking padded tokens, utilize an input tensor that indicates which tokens are not padded (binary matrix). Conversely, the mask for future tokens can be computed internally within the attention mechanism module.\n",
    "3. Revisit the test outlined earlier, and subsequently, formulate a test to verify the accuracy of your attention mechanism implementation.\n",
    "4. Using the test provided 'practical_2_test.py' test your attention machanism implementation. (Ensure that your attention mechanism has the following inputs: query, key, value, mask=None)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75c5eb3c0ff069",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1: Summary:\n",
    "Padding Mask: Prevents attention to padded tokens that don't carry meaningful information.\n",
    "Future Mask: Prevents the model from looking at future tokens in tasks like sequence generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
