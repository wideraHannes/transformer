{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical VII\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "25-29.11.2024\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will combine the word embedding layer, positional encoding layer, and encoder and decoder layers from previous practicals to implement a transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20b8711fe743b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Study the model in the paper [Attention is all you need](https://arxiv.org/abs/1706.03762). Write down the structure of the proposed model.\n",
    "2. Study the section on word embeddings and pay close attention to the parameter sharing. Explain the benefits of parameter sharing in the transformer model.\n",
    "3. Based on your implementations of all the components, implement a transformer model. Use the pytorch `nn.Module` class to implement the model. Your model should be configurable with the following parameters:\n",
    "    - `vocab_size`: The size of the vocabulary\n",
    "    - `d_model`: The dimensionality of the embedding layer\n",
    "    - `n_heads`: The number of heads in the multi-head attention layers\n",
    "    - `num_encoder_layers`: The number of encoder layers\n",
    "    - `num_decoder_layers`: The number of decoder layers\n",
    "    - `dim_feedforward`: The dimensionality of the feedforward layer\n",
    "    - `dropout`: The dropout probability\n",
    "    - `max_len`: The maximum length of the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ed5c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21d8c154",
   "metadata": {},
   "source": [
    "## Parameter Sharing\n",
    "\n",
    "\n",
    "In the Attention Is All You Need architecture, parameter sharing is used primarily in the following components:\n",
    "\n",
    "Input and Output Embeddings: The same embedding matrix is used for both the input and output tokens, tying these weights together. This ensures that the embeddings share a common semantic space, aiding in consistent token representation.\n",
    "\n",
    "Positional Encodings: Although not learned parameters, the positional encoding vectors are shared across the model layers and across all tokens in a sequence, ensuring the positional information is consistent throughout the architecture.\n",
    "\n",
    "Self-Attention Mechanism: While the weights for query, key, and value projections are not shared across layers, the same learned weights are used for all tokens within a single layer. This token-level sharing enforces consistency in how the model interprets relationships.\n",
    "\n",
    "Decoder Cross-Attention: In the decoder, the weights for attention mechanisms over the encoder's output are shared across all decoder layers.\n",
    "\n",
    "These design choices, particularly the embedding sharing, contribute to the model's efficiency and its capacity to generalize effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
