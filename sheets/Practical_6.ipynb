{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical VI\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "11-22.11.2024\n",
    "\n",
    "---\n",
    "In the previous practical, we implemented the transformer encoder layer. In this practical we will implement the transformer decoder layer, including future masking a encoder cross attention.\n",
    "\n",
    "**Note:** Test 2 will cover Practicals 3-6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3becedbe2f606",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Transformer Decoder Layer\n",
    "\n",
    "The transformer decoder layer is very similar to the encoder layer, except for two key differences:\n",
    "\n",
    "1. The decoder layer utilises a future mask in the self attention component.\n",
    "2. The decoder layer utilises an encoder cross attention component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20b8711fe743b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Implement a transformer decoder layer. The layer should consist of a multi-head self-attention layer (with future masking), a multi-head encoder cross attention layer, and a positional wise feed forward layer. Each of these layers should be followed by a residual connection, layer normalisation layer and dropout. (Hint: it is important to use two independent layer normalisation layers following the multi-head self-attention layer and the positional wise feed forward layer)\n",
    "2. Using the tests provided, verify that your implementations are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8e9c54ce28429",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
