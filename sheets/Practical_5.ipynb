{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical V\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "4-8.11.2024\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will implement the multi-head attention and a layer of the transformer encoder. This layer will consist of a multi-head self-attention layer, a residual connection, a layer normalisation layer, and a positional wise feed forward layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3becedbe2f606",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. The Multi-Head Attention Layer\n",
    "\n",
    "The multi-head attention layer (as defined [here](https://arxiv.org/abs/1706.03762)) is a layer that takes a query, key, and value as input and returns an output. The multi-head attention projects each of the inputs to lower dimension features for each of the attention heads. The attention head computes the attention between the query and key and uses the attention weights to compute a weighted sum of the values. Their outputs are then concatenated and projected to the output dimension. The multi-head attention layer is defined as follows:\n",
    "\n",
    "$$ MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) $$\n",
    "\n",
    "and $W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$, and $W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$ are learned linear projections. $d_k$ and $d_v$ are the dimension of the key and value vectors respectively. $h$ is the number of attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766fa4f4d2cab85",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Residual Connections and Layer Normalisation\n",
    "\n",
    "In the transformer model, residual connections help prevent the loss of information, mitigate the vanishing gradient problem, and enable the training of deeper, more efficient networks. Layer normalization in neural networks helps avoid internal covariate shifts by normalizing the inputs within a layer, ensuring consistent distribution of inputs during training, this stabilizes the learning process and improves training speed and model performance.\n",
    "\n",
    "The residual connection and layer normalization layer is defined as follows:\n",
    "\n",
    "$$ LayerNorm(x + Sublayer(x)) $$\n",
    "\n",
    "where $Sublayer(x)$ is the function implemented by the sublayer (e.g. multi-head attention or feed forward layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20b8711fe743b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Study the position wise feed forward layer proposed in the paper [Attention is all you need](https://arxiv.org/abs/1706.03762). Write down the equation for this layer and provide an explanation of the function of this layer in a transformer model.\n",
    "2. Implement the positional wise feed forward layer using the [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) function in PyTorch.\n",
    "3. Implement the multi-head attention layer using the [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) function in PyTorch. (Hint: the projections do not have a bias component)\n",
    "4. Write down the equation for the layer normalization layer and provide an explanation of the function of this layer in a transformer model.\n",
    "5. Implement a transformer encoder layer. The layer should consist of a multi-head self-attention layer, a residual connection, a layer normalisation layer, and a positional wise feed forward layer and a second residual connection and layer normalisation layer. (Hint: it is important to use two independent layer normalisation layers following the multi-head self-attention layer and the position wise feed forward layer. Further, as in the Transformers is all you need model, our layer should include dropout after the multi-head attention and position wise feed forward layers.)\n",
    "6. Using the tests provided, verify that your implementations are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85928cee67e5eb0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The position-wise feed-forward layer is defined as:\n",
    "\n",
    "$ \\text{FFN}(x) = ReLU(0, xW_1 + b_1)W_2 + b_2 $\n",
    "\n",
    "where:\n",
    "\n",
    "( x ) is the input tensor.\n",
    "( W_1 ) and ( W_2 ) are weight matrices.\n",
    "( b_1 ) and ( b_2 ) are bias vectors.\n",
    "( \\max(0, \\cdot) ) represents the ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e94b1",
   "metadata": {},
   "source": [
    "Layer normalization is defined as:\n",
    "\n",
    "$\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta $\n",
    "\n",
    "where:\n",
    "\n",
    "( x ) is the input tensor.\n",
    "( \\mu ) is the mean of the input tensor.\n",
    "( \\sigma^2 ) is the variance of the input tensor.\n",
    "( \\epsilon ) is a small constant to avoid division by zero.\n",
    "( \\gamma ) and ( \\beta ) are learnable parameters.\n",
    "Explanation:\n",
    "\n",
    "Layer normalization normalizes the inputs across the features for each data point independently. This helps in stabilizing the learning process and improving the training speed and model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
